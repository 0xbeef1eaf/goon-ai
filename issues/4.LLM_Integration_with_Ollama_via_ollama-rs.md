## Objective
Implement LLM integration using ollama-rs to communicate with a local Ollama server, send prompts with SDK definitions, and receive TypeScript code responses. **Assets are NOT included in prompts** to reduce context size.
## Module Structure
```mermaid
graph TB
    subgraph "src/llm/"
        LLMMod[mod.rs]
        Client[client.rs]
        Prompt[prompt.rs]
        Conv[conversation.rs]
    end

    Client --> Conv
    Prompt --> Conv
    LLMMod --> Client
    LLMMod --> Prompt
```
## LLM Integration Flow
```mermaid
sequenceDiagram
    participant App as Main Loop
    participant Builder as Prompt Builder
    participant Client as LLM Client
    participant Ollama as Ollama Server

    App->>Builder: Build prompt
    Builder->>Builder: Add system prompt
    Builder->>Builder: Add mood context
    Builder->>Builder: Add SDK definitions
    Builder->>Builder: Add conversation history
    Note over Builder: NO asset list added

    Builder->>App: Complete prompt
    App->>Client: Send prompt
    Client->>Ollama: API request

    alt Success
        Ollama->>Client: TypeScript response
        Client->>App: Parsed code
    else Error
        Ollama->>Client: Error
        Client->>App: Connection error
    end
```
## Prompt Construction (Simplified - No Assets)
```mermaid
flowchart TD
    Start([Build Prompt]) --> SysPrompt[Add System Prompt<br/>from pack config]
    SysPrompt --> MoodPrompt[Add Mood Prompt<br/>if mood active]

    MoodPrompt --> SDK[Add SDK Definitions<br/>based on permissions]

    SDK --> History[Add Conversation History<br/>recent messages]

    History --> Errors{Previous Error?}
    Errors -->|Yes| AddError[Add Error Details<br/>for retry]
    Errors -->|No| UserProfile[Add User Profile]

    AddError --> FinalPrompt[Complete Prompt]
    UserProfile --> FinalPrompt

    FinalPrompt --> CheckSize{Token Count OK?}
    CheckSize -->|Too large| Truncate[Truncate Old History]
    CheckSize -->|OK| Ready([Prompt Ready])

    Truncate --> CheckSize

    Note1[Assets NOT included<br/>Significantly reduced context]
```
## Prompt Template Structure
```mermaid
graph TB
    subgraph "Prompt Components"
        A[System Prompt]
        B[Mood Description]
        C[SDK API]
        D[User Context]
        E[History]
        F[Error Feedback]
    end

    A -->|describes role| Combined
    B -->|"current mood: Nature"| Combined
    C -->|"image.show, video.play"| Combined
    D -->|user preferences| Combined
    E -->|recent actions| Combined
    F -->|if retrying| Combined

    Combined[Full Prompt] --> LLM[Send to Ollama]

    style A fill:#e1f5e1
    style B fill:#e1f5e1
    style C fill:#ffe1e1
    style D fill:#e1e5f5
    style E fill:#f5f5e1
    style F fill:#ffe1e1
```
## Example Prompt Structure
```markdown
# System Prompt (from pack)
You are an AI assistant designed to help test the functionality of goon.ai.
You can display images, play videos, and play audio using the provided SDK.
# Current Mood
The user's current mood is: **Nature**
Focus on showing content related to nature, landscapes, and outdoor environments.
# Available SDK Functions
```typescript
// Display a random image based on current mood
async function image.show(options?: ImageOptions): Promise<WindowHandle>;
// Play a random video based on current mood
async function video.play(options?: VideoOptions): Promise<WindowHandle>;
// Play random audio based on current mood
async function audio.play(options?: AudioOptions): Promise<AudioHandle>;
// Show text prompt
async function prompt.show(text: string, options?: PromptOptions): Promise<WindowHandle>;
```
# User Profile
Name: Joe Bloggs
Gender: male
# Recent History
[Previous interactions...]
# Your Task
Generate TypeScript code using the SDK functions above to interact with the user.
```
## Conversation Management
```mermaid
classDiagram
    class ConversationManager {
        +VecDeque~Message~ history
        +usize max_history
        +add_user_message(content)
        +add_assistant_message(content)
        +add_system_message(content)
        +add_error(error)
        +get_history() Vec~Message~
        +clear()
        +truncate_old()
    }

    class Message {
        +MessageType type
        +String content
        +Instant timestamp
        +usize token_estimate
    }

    class MessageType {
        <<enumeration>>
        System
        User
        Assistant
        Error
        ExecutionResult
    }

    ConversationManager --> Message
    Message --> MessageType
```
## LLM Client Implementation
```mermaid
classDiagram
    class LLMClient {
        +Ollama client
        +String host
        +connect() Result
        +health_check() Result
        +generate(prompt, config) Result~String~
        +pull_model(model) Result
        -parse_response(response) String
    }

    class LLMConfig {
        +String model
        +f32 temperature
        +u32 max_tokens
        +f32 top_p
        +f32 frequency_penalty
        +f32 presence_penalty
        +Vec~String~ stop_sequences
    }

    class PromptBuilder {
        +build_prompt(context) String
        +add_system_prompt(pack)
        +add_mood_context(mood)
        +add_sdk_definitions(permissions)
        +add_conversation_history(history)
        +add_error_context(error)
        +estimate_tokens() usize
    }

    LLMClient --> LLMConfig : uses
    PromptBuilder --> LLMConfig : configures
```
## Error Retry Flow
```mermaid
stateDiagram-v2
    [*] --> SendPrompt: Build prompt (no assets)
    SendPrompt --> WaitResponse: Request sent

    WaitResponse --> ParseCode: Response received
    WaitResponse --> ConnectionError: Network error

    ConnectionError --> Backoff: Log error
    Backoff --> SendPrompt: Retry after delay

    ParseCode --> ExtractTS: Parse TypeScript
    ExtractTS --> [*]: Code extracted
```
## SDK Definitions in Prompt
The LLM receives simple, asset-free API definitions:
```typescript
/**
 * Image display functions
 * Images are automatically selected based on current mood
 */
namespace image {
    /**
     * Show a random image matching the current mood
     * @param options Display options
     * @returns Window handle for the displayed image
     */
    function show(options?: {
        duration?: number;      // Auto-close after seconds
        opacity?: number;       // 0.0 to 1.0
        position?: { x: number; y: number };
        alwaysOnTop?: boolean;
        clickThrough?: boolean;
    }): Promise<WindowHandle>;
}
/**
 * Video playback functions
 * Videos are automatically selected based on current mood
 */
namespace video {
    function play(options?: VideoOptions): Promise<WindowHandle>;
}
// etc...
```
## Tasks
### 1. LLM Module Structure
- [ ] Create LLM module (see diagram)
- [ ] Define module exports
### 2. Ollama Client Setup
- [ ] Implement `LLMClient`
- [ ] Health check/ping
- [ ] Automatic model pulling
### 3. Prompt Construction (Simplified)
- [ ] Build system prompt from pack
- [ ] Add mood description (NOT asset list)
- [ ] Include SDK definitions (filtered by permissions)
- [ ] Add user profile
- [ ] Add conversation history
- [ ] **Exclude asset lists to reduce context**
### 4. Conversation Management
- [ ] Implement conversation history
- [ ] Track messages with types
- [ ] Context window management
- [ ] Truncate old messages
### 5. Request/Response Handling
- [ ] Send requests with pack LLM settings
- [ ] Parse TypeScript from response
- [ ] Extract code blocks
- [ ] Handle markdown formatting
### 6. Error Feedback Loop
- [ ] Format compilation errors for LLM
- [ ] Format runtime errors for LLM
- [ ] Include in next prompt
- [ ] Limit retry attempts
### 7. Dependencies to Add
```toml
[dependencies]
ollama-rs = "0.2"
reqwest = "0.12"
```
### 8. Configuration
- [ ] Support pack LLM settings
- [ ] Model selection
- [ ] Temperature, top_p, etc.
- [ ] Stop sequences
## Context Size Comparison
```mermaid
graph LR
    subgraph "OLD Approach (Large Context)"
        A1[System: 500 tokens]
        A2[SDK: 1000 tokens]
        A3[Assets: 3000 tokens]
        A4[History: 1000 tokens]
        Total1[Total: 5500 tokens]
    end

    subgraph "NEW Approach (Small Context)"
        B1[System: 500 tokens]
        B2[SDK: 1000 tokens]
        B3[Mood: 50 tokens]
        B4[History: 1000 tokens]
        Total2[Total: 2550 tokens]
    end

    A1 --> Total1
    A2 --> Total1
    A3 --> Total1
    A4 --> Total1

    B1 --> Total2
    B2 --> Total2
    B3 --> Total2
    B4 --> Total2

    style A3 fill:#ffcccc
    style B3 fill:#ccffcc
```
## Acceptance Criteria
- [ ] Can connect to Ollama server
- [ ] Successfully sends prompts without asset lists
- [ ] Receives and parses TypeScript responses
- [ ] Handles errors and formats them for retry
- [ ] Maintains conversation context
- [ ] Respects pack's LLM configuration
- [ ] Gracefully handles Ollama being offline
- [ ] Context size significantly reduced (no assets)
- [ ] Mood context included in prompts
## Dependencies
```mermaid
graph LR
    Issue2[#2 Core Init] -->|required| This[Issue #4]
    Issue8[#8 Permissions] -->|required| This
    This -->|required by| Issue16[#16 Main Loop
